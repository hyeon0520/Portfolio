{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16b4940a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FromSingleFileMixin' from 'diffusers.loaders' (c:\\Users\\user\\anaconda3\\envs\\mygp\\Lib\\site-packages\\diffusers\\loaders\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdiffusers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LTXConditionPipeline, LTXLatentUpsamplePipeline\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdiffusers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipelines\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mltx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipeline_ltx_condition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LTXVideoCondition\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdiffusers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_to_video, load_image, load_video\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\mygp\\Lib\\site-packages\\diffusers\\pipelines\\ltx\\pipeline_ltx_condition.py:25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiPipelineCallbacks, PipelineCallback\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_processor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PipelineImageInput\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FromSingleFileMixin, LTXVideoLoraLoaderMixin\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautoencoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoencoderKLLTXVideo\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LTXVideoTransformer3DModel\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'FromSingleFileMixin' from 'diffusers.loaders' (c:\\Users\\user\\anaconda3\\envs\\mygp\\Lib\\site-packages\\diffusers\\loaders\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import LTXConditionPipeline, LTXLatentUpsamplePipeline\n",
    "from diffusers.pipelines.ltx.pipeline_ltx_condition import LTXVideoCondition\n",
    "from diffusers.utils import export_to_video, load_image, load_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5fbdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = LTXConditionPipeline.from_pretrained(\"Lightricks/LTX-Video-0.9.7-dev\", torch_dtype=torch.bfloat16)\n",
    "pipe_upsample = LTXLatentUpsamplePipeline.from_pretrained(\"Lightricks/ltxv-spatial-upscaler-0.9.7\", vae=pipe.vae, torch_dtype=torch.bfloat16)\n",
    "pipe.to(\"cuda\")\n",
    "pipe_upsample.to(\"cuda\")\n",
    "pipe.vae.enable_tiling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3da991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_nearest_resolution_acceptable_by_vae(height, width):\n",
    "    height = height - (height % pipe.vae_spatial_compression_ratio)\n",
    "    width = width - (width % pipe.vae_spatial_compression_ratio)\n",
    "    return height, width\n",
    "\n",
    "image_path = (\"C:/Users/user/Desktop/woohyeon/Portfolio/2025.06.30~07.11 기업인턴 (주)Zento/generate-project/image/theman.jpg\")\n",
    "image = Image.open(image_path)\n",
    "video = load_video(export_to_video([image])) # compress the image using video compression as the model was trained on videos\n",
    "condition1 = LTXVideoCondition(video=video, frame_index=0)\n",
    "\n",
    "prompt = \"A cute little penguin takes out a book and starts reading it\"\n",
    "negative_prompt = \"worst quality, inconsistent motion, blurry, jittery, distorted\"\n",
    "expected_height, expected_width = 480, 832\n",
    "downscale_factor = 2 / 3\n",
    "num_frames = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee6f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1. Generate video at smaller resolution\n",
    "downscaled_height, downscaled_width = int(expected_height * downscale_factor), int(expected_width * downscale_factor)\n",
    "downscaled_height, downscaled_width = round_to_nearest_resolution_acceptable_by_vae(downscaled_height, downscaled_width)\n",
    "latents = pipe(\n",
    "    conditions=[condition1],\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    width=downscaled_width,\n",
    "    height=downscaled_height,\n",
    "    num_frames=num_frames,\n",
    "    num_inference_steps=30,\n",
    "    generator=torch.Generator().manual_seed(0),\n",
    "    output_type=\"latent\",\n",
    ").frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cf0a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2. Upscale generated video using latent upsampler with fewer inference steps\n",
    "# The available latent upsampler upscales the height/width by 2x\n",
    "upscaled_height, upscaled_width = downscaled_height * 2, downscaled_width * 2\n",
    "upscaled_latents = pipe_upsample(\n",
    "    latents=latents,\n",
    "    output_type=\"latent\"\n",
    ").frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0574e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3. Denoise the upscaled video with few steps to improve texture (optional, but recommended)\n",
    "video = pipe(\n",
    "    conditions=[condition1],\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    width=upscaled_width,\n",
    "    height=upscaled_height,\n",
    "    num_frames=num_frames,\n",
    "    denoise_strength=0.4,  # Effectively, 4 inference steps out of 10\n",
    "    num_inference_steps=10,\n",
    "    latents=upscaled_latents,\n",
    "    decode_timestep=0.05,\n",
    "    image_cond_noise_scale=0.025,\n",
    "    generator=torch.Generator().manual_seed(0),\n",
    "    output_type=\"pil\",\n",
    ").frames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b70d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4. Downscale the video to the expected resolution\n",
    "video = [frame.resize((expected_width, expected_height)) for frame in video]\n",
    "\n",
    "export_to_video(video, \"output.mp4\", fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e805aa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c6fca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
